{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T16:34:48.775722Z",
     "start_time": "2018-03-01T16:34:42.178998Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import numba\n",
    "import os,sys\n",
    "\n",
    "def LoadData(InputDir):\n",
    "    \"\"\"\"\"\"\n",
    "    ## load raw data\n",
    "    data = {\n",
    "        'train': pd.read_csv('%s/round1_ijcai_18_train_20180301.txt' % InputDir, sep= ' '),\n",
    "        'test': pd.read_csv('%s/round1_ijcai_18_test_a_20180301.txt' % InputDir, sep= ' '),\n",
    "    }\n",
    "    return data\n",
    "\n",
    "DataBaseDir = '../../data'\n",
    "DataSet = LoadData('%s/raw' % DataBaseDir)\n",
    "for mod in ['train', 'test']:\n",
    "    DataSet[mod]['context_timestamp'] = DataSet[mod]['context_timestamp'].apply(lambda x: datetime.datetime.fromtimestamp(x))\n",
    "    DataSet[mod] = DataSet[mod].sort_values(by= 'context_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T16:34:51.333371Z",
     "start_time": "2018-03-01T16:34:48.777743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance_id                           int64\n",
      "item_id                               int64\n",
      "item_category_list                   object\n",
      "item_property_list                   object\n",
      "item_brand_id                         int64\n",
      "item_city_id                          int64\n",
      "item_price_level                      int64\n",
      "item_sales_level                      int64\n",
      "item_collected_level                  int64\n",
      "item_pv_level                         int64\n",
      "user_id                               int64\n",
      "user_gender_id                        int64\n",
      "user_age_level                        int64\n",
      "user_occupation_id                    int64\n",
      "user_star_level                       int64\n",
      "context_id                            int64\n",
      "context_timestamp            datetime64[ns]\n",
      "context_page_id                       int64\n",
      "predict_category_property            object\n",
      "shop_id                               int64\n",
      "shop_review_num_level                 int64\n",
      "shop_review_positive_rate           float64\n",
      "shop_star_level                       int64\n",
      "shop_score_service                  float64\n",
      "shop_score_delivery                 float64\n",
      "shop_score_description              float64\n",
      "is_trade                              int64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "train size 478138, test size 18371\n",
      "\n",
      "\n",
      "---- train part ----\n",
      "3844    2018-09-18 00:00:01\n",
      "24046   2018-09-18 00:00:04\n",
      "59329   2018-09-18 00:00:05\n",
      "70767   2018-09-18 00:00:11\n",
      "76650   2018-09-18 00:00:14\n",
      "Name: context_timestamp, dtype: datetime64[ns]\n",
      "\n",
      "\n",
      "457234   2018-09-24 23:59:32\n",
      "461680   2018-09-24 23:59:33\n",
      "430665   2018-09-24 23:59:44\n",
      "470125   2018-09-24 23:59:46\n",
      "422581   2018-09-24 23:59:47\n",
      "Name: context_timestamp, dtype: datetime64[ns]\n",
      "---- test part ----\n",
      "6594    2018-09-25 00:00:02\n",
      "2782    2018-09-25 00:00:20\n",
      "9049    2018-09-25 00:00:25\n",
      "13921   2018-09-25 00:00:29\n",
      "6324    2018-09-25 00:00:31\n",
      "Name: context_timestamp, dtype: datetime64[ns]\n",
      "\n",
      "\n",
      "16071   2018-09-25 23:59:02\n",
      "10893   2018-09-25 23:59:06\n",
      "11766   2018-09-25 23:59:08\n",
      "14325   2018-09-25 23:59:13\n",
      "16626   2018-09-25 23:59:25\n",
      "Name: context_timestamp, dtype: datetime64[ns]\n",
      "\n",
      "\n",
      "---- train part ----\n",
      "item_id unique length 10075\n",
      "user_id unique length 197694\n",
      "context_id unique length 478111\n",
      "shop_id unique length 3959\n",
      "---- test part ----\n",
      "item_id unique length 3695\n",
      "user_id unique length 13573\n",
      "context_id unique length 18371\n",
      "shop_id unique length 2015\n",
      "\n",
      "\n",
      "repeated context size 27\n",
      "after removing repeated context, size 478086\n",
      "\n",
      "\n",
      "item_brand_id null size 473\n",
      "item_city_id null size 277\n",
      "item_sales_level null size 913\n",
      "user_gender_id null size 12902\n",
      "user_age_level null size 964\n",
      "user_occupation_id null size 964\n",
      "user_star_level null size 964\n",
      "shop_review_positive_rate null size 7\n",
      "shop_score_service null size 59\n",
      "shop_score_delivery null size 59\n",
      "shop_score_description null size 59\n",
      "\n",
      "\n",
      "day 0 positive rate 0.02\n",
      "day 1 positive rate 0.02\n",
      "day 2 positive rate 0.02\n",
      "day 3 positive rate 0.02\n",
      "day 4 positive rate 0.02\n",
      "day 5 positive rate 0.02\n",
      "day 6 positive rate 0.02\n",
      "\n",
      "\n",
      "hour 0(2018/9/18) positive rate 0.02\n",
      "hour 1(2018/9/18) positive rate 0.02\n",
      "hour 2(2018/9/18) positive rate 0.03\n",
      "hour 3(2018/9/18) positive rate 0.02\n",
      "hour 4(2018/9/18) positive rate 0.04\n",
      "hour 5(2018/9/18) positive rate 0.02\n",
      "hour 6(2018/9/18) positive rate 0.02\n",
      "hour 7(2018/9/18) positive rate 0.03\n",
      "hour 8(2018/9/18) positive rate 0.03\n",
      "hour 9(2018/9/18) positive rate 0.03\n",
      "hour 10(2018/9/18) positive rate 0.03\n",
      "hour 11(2018/9/18) positive rate 0.02\n",
      "hour 12(2018/9/18) positive rate 0.02\n",
      "hour 13(2018/9/18) positive rate 0.02\n",
      "hour 14(2018/9/18) positive rate 0.02\n",
      "hour 15(2018/9/18) positive rate 0.02\n",
      "hour 16(2018/9/18) positive rate 0.02\n",
      "hour 17(2018/9/18) positive rate 0.02\n",
      "hour 18(2018/9/18) positive rate 0.02\n",
      "hour 19(2018/9/18) positive rate 0.02\n",
      "hour 20(2018/9/18) positive rate 0.02\n",
      "hour 21(2018/9/18) positive rate 0.02\n",
      "hour 22(2018/9/18) positive rate 0.02\n",
      "hour 23(2018/9/18) positive rate 0.01\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# check #\n",
    "#########\n",
    "print(DataSet['train'].dtypes)\n",
    "## sample size\n",
    "print('\\n')\n",
    "print('train size %s, test size %s' % (len(DataSet['train']), len(DataSet['test'])))\n",
    "## time range\n",
    "print('\\n')\n",
    "for mod in ['train', 'test']:\n",
    "    print('---- %s part ----' % mod)\n",
    "    print(DataSet[mod]['context_timestamp'].head(5))\n",
    "    print('\\n')\n",
    "    print(DataSet[mod]['context_timestamp'].tail(5))\n",
    "## unique objects\n",
    "print('\\n')\n",
    "for mod in ['train', 'test']:\n",
    "    print('---- %s part ----' % mod)\n",
    "    for ob in ['item_id', 'user_id', 'context_id', 'shop_id']:\n",
    "        print('%s unique length %s' % (ob, len(DataSet[mod][ob].unique())))\n",
    "## remove repeated context\n",
    "RepeatedContext = list()\n",
    "UniqueContext = set()\n",
    "for v in DataSet['train']['context_id'].values:\n",
    "    if(v in UniqueContext):\n",
    "        RepeatedContext.append(v)\n",
    "    else:\n",
    "        UniqueContext.add(v)\n",
    "print('\\n')\n",
    "print('repeated context size %s' % len(RepeatedContext))\n",
    "DataSet['train'] = DataSet['train'][~DataSet['train']['context_id'].isin(RepeatedContext)]\n",
    "print('after removing repeated context, size %s' % len(DataSet['train']))\n",
    "## null size\n",
    "print('\\n')\n",
    "for col in DataSet['train'].columns:\n",
    "    coldt = DataSet['train'][col].dtype.name\n",
    "    if((coldt == 'int64') or (coldt == 'float64')):\n",
    "        nullsize = len(DataSet['train'][DataSet['train'][col] == -1])\n",
    "        if(nullsize > 0):\n",
    "            print('%s null size %s' % (col, nullsize))\n",
    "## target rate\n",
    "print('\\n')\n",
    "for mod in ['train', 'test']:\n",
    "    DataSet[mod]['hour'] = DataSet[mod]['context_timestamp'].dt.hour ## new column, feature\n",
    "    DataSet[mod]['date'] = DataSet[mod]['context_timestamp'].dt.date ## new column\n",
    "## \n",
    "for i in range(7):\n",
    "    daydf = DataSet['train'][DataSet['train']['date'] == (datetime.date(2018, 9, 18) + datetime.timedelta(days= i))]\n",
    "    posnum = len(daydf[daydf['is_trade'] == 1])\n",
    "    print('day %s positive rate %.2f' % (i, (posnum/len(daydf))))\n",
    "print('\\n')\n",
    "daydf = DataSet['train'][DataSet['train']['date'] == (datetime.date(2018, 9, 18))]\n",
    "for i in range(24):\n",
    "    hourdf = daydf[daydf['hour'] == i]\n",
    "    posnum = len(hourdf[hourdf['is_trade'] == 1])\n",
    "    print('hour %s(2018/9/18) positive rate %.2f' % (i, (posnum/len(hourdf))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T16:34:53.205874Z",
     "start_time": "2018-03-01T16:34:51.334904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---- train ----\n",
      "\n",
      "\n",
      "7908382889764677758    478086\n",
      "Name: category_0, dtype: int64\n",
      "\n",
      "\n",
      "8277336076276184272    150771\n",
      "5755694407684602296    102492\n",
      "509660095530134768      75416\n",
      "5799347067982556520     72009\n",
      "7258015885215914736     53029\n",
      "2011981573061447208      9563\n",
      "8710739180200009128      7084\n",
      "3203673979138763595      2246\n",
      "2642175453151805566      2029\n",
      "2436715285093487584      1966\n",
      "4879721024980945592      1207\n",
      "1968056100269760729       186\n",
      "22731265849056483          88\n",
      "Name: category_1, dtype: int64\n",
      "\n",
      "\n",
      "                       476057\n",
      "8868887661186419229      1906\n",
      "6233669177166538628       123\n",
      "Name: category_2, dtype: int64\n",
      "\n",
      "\n",
      "---- test ----\n",
      "\n",
      "\n",
      "7908382889764677758    18371\n",
      "Name: category_0, dtype: int64\n",
      "\n",
      "\n",
      "8277336076276184272    6098\n",
      "5755694407684602296    4033\n",
      "509660095530134768     2932\n",
      "5799347067982556520    2726\n",
      "7258015885215914736    1464\n",
      "2011981573061447208     373\n",
      "8710739180200009128     304\n",
      "3203673979138763595     223\n",
      "2642175453151805566      79\n",
      "2436715285093487584      72\n",
      "4879721024980945592      55\n",
      "1968056100269760729      10\n",
      "22731265849056483         2\n",
      "Name: category_1, dtype: int64\n",
      "\n",
      "\n",
      "                       18292\n",
      "8868887661186419229       75\n",
      "6233669177166538628        4\n",
      "Name: category_2, dtype: int64\n",
      "\n",
      "\n",
      "---- train ----\n",
      "item_brand_id unique size 2055\n",
      "item_city_id unique size 128\n",
      "item_price_level unique size 14\n",
      "item_sales_level unique size 18\n",
      "item_collected_level unique size 18\n",
      "item_pv_level unique size 22\n",
      "\n",
      "\n",
      "---- test ----\n",
      "item_brand_id unique size 1101\n",
      "item_city_id unique size 99\n",
      "item_price_level unique size 10\n",
      "item_sales_level unique size 18\n",
      "item_collected_level unique size 18\n",
      "item_pv_level unique size 21\n"
     ]
    }
   ],
   "source": [
    "#### item part\n",
    "## item_category_list, dependently joined, treat it as an entire entity\n",
    "@numba.jit\n",
    "def ApplySplitCategory(colvals):\n",
    "    \"\"\"\"\"\"\n",
    "    n = len(colvals)\n",
    "    result = np.empty((n, 3), dtype= 'object')\n",
    "    for i in range(n):\n",
    "        cate_list = colvals[i].split(';')\n",
    "        for j in range(3):\n",
    "            if(j < len(cate_list)):\n",
    "                result[i, j] = cate_list[j]\n",
    "            else:\n",
    "                result[i, j] = ''\n",
    "    return result\n",
    "\n",
    "for mod in ['train', 'test']:\n",
    "    print('\\n')\n",
    "    print('---- %s ----' % mod)\n",
    "    tmp = ApplySplitCategory(DataSet[mod]['item_category_list'].values) \n",
    "    tmpdf = pd.DataFrame(data= tmp, index= DataSet[mod].index, columns=['category_0', 'category_1', 'category_2'])\n",
    "    DataSet[mod] = pd.concat([DataSet[mod], tmpdf], axis= 1, ignore_index= False)\n",
    "    for col in ['category_0', 'category_1', 'category_2']:\n",
    "        print('\\n')\n",
    "        print(DataSet[mod][col].value_counts())\n",
    "# ## item_property_list, independently joined, treat it seperately\n",
    "# for mod in ['train', 'test']:\n",
    "#     print('\\n')\n",
    "#     print('---- %s ----' % mod)\n",
    "#     tmp = ApplySplitCategory(DataSet[mod]['item_property_list'].values)\n",
    "#     tmpdf = pd.DataFrame(data= tmp, index= range(len(tmp)), columns=['property_0', 'property_1', 'property_2'])\n",
    "#     df = pd.concat([DataSet[mod], tmpdf], axis= 1)\n",
    "#     for col in ['property_0', 'property_1', 'property_2']:\n",
    "#         print('\\n')\n",
    "#         print(df[col].value_counts())\n",
    "\n",
    "## item_brand_id, item_city_id\n",
    "item_cate_feat = ['item_brand_id', 'item_city_id', 'item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level']\n",
    "for mod in ['train', 'test']:\n",
    "    print('\\n')\n",
    "    print('---- %s ----' % mod)\n",
    "    for col in item_cate_feat:\n",
    "        print('%s unique size %s' % (col, len(DataSet[mod][col].unique())))\n",
    "        DataSet[mod][col] = DataSet[mod][col].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T16:34:53.317434Z",
     "start_time": "2018-03-01T16:34:53.207742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---- train ----\n",
      "user_gender_id unique size 4\n",
      "user_age_level unique size 9\n",
      "user_occupation_id unique size 5\n",
      "user_star_level unique size 12\n",
      "\n",
      "\n",
      "---- test ----\n",
      "user_gender_id unique size 4\n",
      "user_age_level unique size 9\n",
      "user_occupation_id unique size 5\n",
      "user_star_level unique size 12\n"
     ]
    }
   ],
   "source": [
    "#### user part\n",
    "## user_gender_id, user_age_level, user_occupation_id, user_star_level\n",
    "user_cate_feat = ['user_gender_id', 'user_age_level', 'user_occupation_id', 'user_star_level']\n",
    "for mod in ['train', 'test']:\n",
    "    print('\\n')\n",
    "    print('---- %s ----' % mod)\n",
    "    for col in user_cate_feat:\n",
    "        print('%s unique size %s' % (col, len(DataSet[mod][col].unique())))\n",
    "        DataSet[mod][col] = DataSet[mod][col].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T16:34:53.374171Z",
     "start_time": "2018-03-01T16:34:53.319180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---- train ----\n",
      "shop_review_num_level unique size 25\n",
      "shop_star_level unique size 22\n",
      "\n",
      "\n",
      "---- test ----\n",
      "shop_review_num_level unique size 21\n",
      "shop_star_level unique size 20\n"
     ]
    }
   ],
   "source": [
    "#### shop part\n",
    "## shop_review_num_level, shop_star_level \n",
    "shop_cate_feat = ['shop_review_num_level', 'shop_star_level']\n",
    "shop_num_feat = ['shop_review_positive_rate', 'shop_score_service', 'shop_score_delivery', 'shop_score_description']\n",
    "for mod in ['train', 'test']:\n",
    "    print('\\n')\n",
    "    print('---- %s ----' % mod)\n",
    "    for col in shop_cate_feat:\n",
    "        print('%s unique size %s' % (col, len(DataSet[mod][col].unique())))\n",
    "        DataSet[mod][col] = DataSet[mod][col].astype('object')\n",
    "#print(DataSet['train'][user_cate_feat].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T16:36:54.293670Z",
     "start_time": "2018-03-01T16:36:51.912235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99921953395\n",
      "0.998632045281\n"
     ]
    }
   ],
   "source": [
    "#### context part\n",
    "predict_cate = set()\n",
    "predict_prop = set()\n",
    "@numba.jit\n",
    "def ApplyPredictCategory(colvals):\n",
    "    \"\"\"\"\"\"\n",
    "    n = len(colvals)\n",
    "    result = np.empty((n, 1), dtype= 'object')\n",
    "    for i in range(n):\n",
    "        if(colvals[i] != '-1'):\n",
    "            cate_prop_list = colvals[i].split(';')\n",
    "            cate_list = []\n",
    "            for cpl in cate_prop_list:\n",
    "                tmp = cpl.split(':')\n",
    "                cate = tmp[0]\n",
    "                cate_list.append(cate)\n",
    "            result[i] = ','.join(cate_list)\n",
    "        else:\n",
    "            result[i] = '-1'\n",
    "    return result\n",
    "\n",
    "@numba.jit\n",
    "def ApplyHitCategory(cate0, cate1, cate2, colvals):\n",
    "    \"\"\"\"\"\"\n",
    "    n = len(colvals)\n",
    "    result = np.zeros((n, 1), dtype= 'int8')\n",
    "    for i in range(n):\n",
    "        if(colvals[i] == '-1'):\n",
    "            result[i] = 1\n",
    "            continue\n",
    "        cate_list = colvals[i].split(',')\n",
    "        if(cate0[i] in cate_list):\n",
    "            result[i] = 1\n",
    "            continue\n",
    "        elif(cate1[i] in cate_list):\n",
    "            result[i] = 1\n",
    "            continue\n",
    "        elif(cate2[i] in cate_list):\n",
    "            result[i] = 1\n",
    "            continue\n",
    "    return result\n",
    "\n",
    "DataSet['train']['predict_category_list'] = ApplyPredictCategory(DataSet['train']['predict_category_property'].values)\n",
    "DataSet['train']['hit_category'] = ApplyHitCategory(DataSet['train']['category_0'].values, \n",
    "                                                    DataSet['train']['category_1'].values, \n",
    "                                                    DataSet['train']['category_2'].values, \n",
    "                                                    DataSet['train']['predict_category_list'].values)\n",
    "posdf = DataSet['train'][DataSet['train']['is_trade'] == 1]\n",
    "posvc = pospdf['hit_category'].value_counts()\n",
    "print(posvc[1]/len(posdf))\n",
    "totalvc = DataSet['train']['hit_category'].value_counts()\n",
    "print(totalvc[1]/len(DataSet['train']))\n",
    "#tmpdf1 = tmpdf[[tmpdf['hit'] == 1]]\n",
    "#print(len(tmpdf1)/len(tmpdf))\n",
    "# 7908382889764677758\n",
    "#         try:\n",
    "#             prop = tmp[1]\n",
    "#             for p in prop.split(','):\n",
    "#                 predict_prop.add(p)\n",
    "#                 if(p in tmpset):\n",
    "#                     print(cate)\n",
    "#                 else:\n",
    "#                     tmpset.add(p)\n",
    "#         except:\n",
    "#             print(tmp)\n",
    "#         predict_cate.add(cate)\n",
    "# print(len(predict_cate))\n",
    "# print(len(predict_prop))\n",
    "# sys.exit(1)\n",
    "# context_cate_feat = ['context_page_id']\n",
    "# for mod in ['train', 'test']:\n",
    "#     print('\\n')\n",
    "#     print('---- %s ----' % mod)\n",
    "#     for col in context_cate_feat:\n",
    "#         print('%s unique size %s' % (col, len(DataSet[mod][col].unique())))\n",
    "#         DataSet[mod][col] = DataSet[mod][col].astype('object')\n",
    "        \n",
    "# pcp = DataSet['train']['item_category_list'][:20].values\n",
    "# print(pcp)\n",
    "#print('------------------')\n",
    "#print(DataSet['train'][['item_category_list', 'item_property_list']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
